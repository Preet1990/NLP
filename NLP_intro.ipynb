{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_intro.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNc0TumtpTQDRQO4I0qj0QU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Preet1990/NLP/blob/main/NLP_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step1: Sentence Segmentation"
      ],
      "metadata": {
        "id": "0bH-SJMmxRIw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa_lDNxtrDmW",
        "outputId": "cde94221-a02c-4513-ead5-98fb7a2c138b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello everyone.', 'Welcome to Tech IS.Tech IS is a global programming school.We are from Silicon Valley, USA.', 'Our Tutors are Professional engineers with extensive work experience in Information Technology Industry hailing from around the world.', 'You are studying NLP article.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "paragraph = \"\"\"Hello everyone. Welcome to Tech IS.Tech IS is a global programming school.We are from Silicon Valley, USA. \n",
        "            Our Tutors are Professional engineers with extensive work experience in Information Technology Industry hailing from around the world.\n",
        "            You are studying NLP article.\"\"\"\n",
        "             \n",
        "# Tokenizing sentences\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step2: Word Tokenization"
      ],
      "metadata": {
        "id": "h0S8U0PVxZYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing words\n",
        "words = nltk.word_tokenize(paragraph)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVj0lAsixOcb",
        "outputId": "c617d2ab-12e5-420d-aadf-c75acf5a7662"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.', 'Welcome', 'to', 'Tech', 'IS.Tech', 'IS', 'is', 'a', 'global', 'programming', 'school.We', 'are', 'from', 'Silicon', 'Valley', ',', 'USA', '.', 'Our', 'Tutors', 'are', 'Professional', 'engineers', 'with', 'extensive', 'work', 'experience', 'in', 'Information', 'Technology', 'Industry', 'hailing', 'from', 'around', 'the', 'world', '.', 'You', 'are', 'studying', 'NLP', 'article', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step3: Identifying Stop Words"
      ],
      "metadata": {
        "id": "WQTrNk57xgu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbSzocgExfkv",
        "outputId": "1486a757-1536-4c6e-e899-b5ba6042a75d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step4: Stemming"
      ],
      "metadata": {
        "id": "ZTLNc_ZtyadJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Stemming\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)   \n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOIqfnsJxk5_",
        "outputId": "1b91f8b8-c220-4bca-8ede-f1b650ebc88a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello everyon .', 'welcom tech is.tech is global program school.w silicon valley , usa .', 'our tutor profession engin extens work experi inform technolog industri hail around world .', 'you studi nlp articl .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Lemmatization"
      ],
      "metadata": {
        "id": "tJcKbuqayc8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import these modules \n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatization\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words) \n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8WhcqIryf3q",
        "outputId": "08889028-457d-4bcc-8694-88557671447b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello everyone .', 'Welcome Tech IS.Tech IS global programming school.We Silicon Valley , USA .', 'Our Tutors Professional engineer extensive work experience Information Technology Industry hailing around world .', 'You studying NLP article .']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words (BoW) model"
      ],
      "metadata": {
        "id": "9lrm_9H1aXI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "paragraph\n",
        "ps = PorterStemmer()\n",
        "wordnet=WordNetLemmatizer()\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "corpus = []\n",
        "for i in range(len(sentences)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)\n",
        "    \n",
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 1500)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af7gqPDYyxto",
        "outputId": "5456267b-6f92-4c28-fc53-4e19c0ec5ee0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 2 0 0 1 1 1 0 0]\n",
            " [1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF (term frequency-inverse document frequency)\n"
      ],
      "metadata": {
        "id": "4o8Riuvwa1UK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning the texts and creat corpus\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "wordnet=WordNetLemmatizer()\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "corpus = []\n",
        "for i in range(len(sentences)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)\n",
        "    \n",
        "# Creating the TF-IDF model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv = TfidfVectorizer()\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIAgVlrCacEh",
        "outputId": "9426fa4c-d5ba-46e3-a201-f574f7fa4da7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.70710678 0.         0.\n",
            "  0.         0.         0.70710678 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.30151134 0.         0.         0.         0.         0.\n",
            "  0.         0.30151134 0.30151134 0.30151134 0.         0.60302269\n",
            "  0.         0.         0.30151134 0.30151134 0.30151134 0.\n",
            "  0.        ]\n",
            " [0.28867513 0.         0.28867513 0.         0.28867513 0.28867513\n",
            "  0.         0.28867513 0.         0.28867513 0.28867513 0.\n",
            "  0.28867513 0.         0.         0.         0.         0.\n",
            "  0.28867513 0.28867513 0.         0.         0.         0.28867513\n",
            "  0.28867513]\n",
            " [0.         0.57735027 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.57735027\n",
            "  0.         0.         0.         0.         0.57735027 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]]\n"
          ]
        }
      ]
    }
  ]
}